# -*- coding: utf-8 -*-
"""Rafinacja Informacji

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10vTxBcnGaM9RYBVlG94X_pJDZ_Grs3eM
"""
# Błachnio, Bujak, 2024 r.


# Stworzony model opiera się na reprezentacji word embedding, w ramach której osadzamy słowa w pewnej relacji
# semantycznej do reszty słów z korpusu. Oznacza to, że każde słowo jest reprezentowane jako wektor, który można
# umieścić w konkretnym miejscu na wykresie. Przyrównując dany wektor do innego wektora, jesteśmy w stanie określić
# relację bliskości znaczeniowej między poszczególnymi wektorami. stanowi to istotny szczegół wyróżniający tę metodę
# reprezentacji tekstu od reprezentacji Bag Of Words czy TF-IDF ktora prezentuje tekst jedynie na podstawie
# występowania słów, nie mierzy jednak kontekstu ani relacji semantycznej między poszczególnymi słowami.

# Zaprezentowany model wykorzystuje prace Williama Shakespeare'a do reprezentacji słów w postaci word embedding.
# W pierwszej kolejności kod wczytuje baze tekstów autora, które następnie rozdziela na linie. Następnie tekst
# musi zostać obrobiony i ztokenizowany. W ramach standaryzacji tekstu zastosowaliśmy konwersję na małe litery
# oraz usunięcie znaków interpunkcyjnych. Jest to najbardziej podstawowa analiza, przeglądając szerszą próbkę
# słów, można spróbować usuwać poszczególne słowa nie niosące znaczenia oraz dodatkowe znaki interpunkcyjne/symbole
# które nie są przez program usuwane. Potem następuje wektoryzacja, czyli przekształcenie słów w wektory o stałej
# długości, które mogą być ze sobą porównywane (główna cecha word embeddingu).

# Po wektoryzacji tekstów kod generuje dane treningowe. W tym celu wykorzystuje metodę "skip-gram", która polega
# na tworzeniu par słowo-kontekst z określonym oknem kontekstowym. Dla każdej pary słowo-kontekst generowane są
# również negatywne próbki, które służą do uczenia modelu rozróżniania rzeczywistych kontekstów od losowych.

# Następnie definiowany jest model Word2Vec, który posiada dwie warstwy embedding: jedną dla słów docelowych i
# jedną dla kontekstów. Model ten uczy się reprezentacji słów w taki sposób, aby słowa występujące w podobnych
# kontekstach miały podobne wektory. Funkcja straty używana do treningu modelu to sigmoid cross-entropy.
# oprócz Word2Vec, istnieją inne modele osadzania słów: GloVe i FastText. GloVe wykorzystuja bardziej globalne dane
# do określenia relacji między słowami i ich występowania, natomiast FastText jest bardziej zaawansowaną metodą
# osadzania słów, opartą na modelowaniu n-gramów.

# Model jest trenowany na przygotowanych danych przez 20 epok (dla 10 epok model nie był wystarczająco dokładny),
# a wyniki treningu są zapisywane do logów TensorBoard, co umożliwia wizualizację postępów treningu. Po zakończeniu
# treningu wagi osadzeń (wektory słów) są zapisywane do plików, żeby można je było później analizować lub wizualizować.


import io
import re
import string
import tqdm

import numpy as np

import tensorflow as tf
from tensorflow.keras import layers

# %load_ext tensorboard

# Ustawienie random seeda i zdefiniowanie Autotune'a dla optymalizacji wydajności zbioru danych
SEED = 42
AUTOTUNE = tf.data.AUTOTUNE

# Tworzenie tabeli sampli
sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)
print(sampling_table)

# stworzenie funkcji do danych treningowych
def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):
  targets, contexts, labels = [], [], []

  # Tworzenie tabeli sampli
  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)

  # Iterowanie przez sekwencje
  for sequence in tqdm.tqdm(sequences):
    # Generowanie pozytywnych par skip-gram
    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(
          sequence,
          vocabulary_size=vocab_size,
          sampling_table=sampling_table,
          window_size=window_size,
          negative_samples=0)

    # Iterowanie przez pary target-kontext
    for target_word, context_word in positive_skip_grams:
      context_class = tf.expand_dims(
          tf.constant([context_word], dtype="int64"), 1)
      # Generowanie negatywnych sampli
      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(
          true_classes=context_class,
          num_true=1,
          num_sampled=num_ns,
          unique=True,
          range_max=vocab_size,
          seed=seed,
          name="negative_sampling")

      # Łączenie kontekstu pozytywnego z negatywnymi samplami
      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)
      label = tf.constant([1] + [0]*num_ns, dtype="int64")

      # Dodawanie docelowego słowa, kontekstów i etykiet do list
      targets.append(target_word)
      contexts.append(context)
      labels.append(label)

  return targets, contexts, labels

# Pobieranie pliku z tekstem Shakespeare'a
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')

# Odczytywanie pliku i dzielenie na linie
with open(path_to_file) as f:
  lines = f.read().splitlines()
for line in lines[:20]:
  print(line)  # Wypisanie pierwszych 20 linii tekstu

# Tworzenie datasetu z linii tekstu i filtrowanie pustych linii
text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))

# Definiowanie funkcji standardyzującej tekst
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  return tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation), '')

# Ustawienie wielkości słownictwa i długości sekwencji
vocab_size = 4096
sequence_length = 10

# Tworzenie warstwy vectorize
vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=vocab_size,
    output_mode='int',
    output_sequence_length=sequence_length)

# Adaptacja warstwy vectorize do datasetu
vectorize_layer.adapt(text_ds.batch(1024))

# Pobieranie słownika z vectorize
inverse_vocab = vectorize_layer.get_vocabulary()
print(inverse_vocab[:20])  # Wypisanie pierwszych 20 słów słownictwa

# Konwersja tekstu na wektory i unbatchowanie
text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()

# Konwersja datasetu na sekwencje
sequences = list(text_vector_ds.as_numpy_iterator())
print(len(sequences))  # Wypisanie liczby sekwencji

# testowe Wypisanie pierwszych 5 sekwencji i odpowiadających im słów
for seq in sequences[:5]:
  print(f"{seq} => {[inverse_vocab[i] for i in seq]}")

# Generowanie danych treningowych
targets, contexts, labels = generate_training_data(
    sequences=sequences,
    window_size=2,
    num_ns=4,
    vocab_size=vocab_size,
    seed=SEED)

# Konwersja list do np.arrays
targets = np.array(targets)
contexts = np.array(contexts)
labels = np.array(labels)

print('\n')
# Wypisanie kształtu tablicy docelowych słów
print(f"targets.shape: {targets.shape}")
# Wypisanie kształtu tablicy kontekstów
print(f"contexts.shape: {contexts.shape}")
# Wypisanie kształtu tablicy etykiet
print(f"labels.shape: {labels.shape}")

# Ustawienie wielkości batcha i buffera
BATCH_SIZE = 1024
BUFFER_SIZE = 10000
# Tworzenie datasetu z tensorów
dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
print(dataset)

# Caching i prefetching datasetu
dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)
print(dataset)  # Wypisanie datasetu

# Definicja klasy modelu Word2Vec
class Word2Vec(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim):
    super(Word2Vec, self).__init__()
    self.target_embedding = layers.Embedding(vocab_size,
                                      embedding_dim,
                                      name="w2v_embedding")
    self.context_embedding = layers.Embedding(vocab_size,
                                       embedding_dim)

  def call(self, pair):
    target, context = pair

    if len(target.shape) == 2:
      target = tf.squeeze(target, axis=1)

    word_emb = self.target_embedding(target)

    context_emb = self.context_embedding(context)

    dots = tf.einsum('be,bce->bc', word_emb, context_emb)

    return dots

# Definiowanie funkcji straty
def custom_loss(x_logit, y_true):
      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)

# Stworzenie modelu Word2Vec
embedding_dim = 128
word2vec = Word2Vec(vocab_size, embedding_dim)
word2vec.compile(optimizer='adam',
                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                 metrics=['accuracy'])

# Stworzenie callbacku do TensorBoard
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")

# Trenowanie modelu Word2Vec
word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])

# Pobranie wag z modelu Word2Vec
weights = word2vec.get_layer('w2v_embedding').get_weights()[0]
vocab = vectorize_layer.get_vocabulary()

# Tworzenie plików do wizualizacji osadzeń w TensorBoard
out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')

# Zapisywanie wektorów i odpowiadających im słów do plików
for index, word in enumerate(vocab):
  if index == 0:
    continue
  vec = weights[index]
  out_v.write('\t'.join([str(x) for x in vec]) + "\n")
  out_m.write(word + "\n")
out_v.close()
out_m.close()

# Próba pobrania plików (w Google Colab)
try:
  from google.colab import files
  files.download('vectors.tsv')
  files.download('metadata.tsv')
except Exception:
  pass

# %tensorboard --logdir logs


#pobrane pliki mozna graficznie zwizualizować na stronie: https://projector.tensorflow.org
